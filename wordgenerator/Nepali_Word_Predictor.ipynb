{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Nepali Word Predictor.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeMcxojBvbyl",
        "outputId": "26cae04a-755d-40b0-d358-0eb54b48105a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQbl5K9FvNle"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import random"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGzJPrz3wE7w"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, embed_dim, hidden_size, output_size,\n",
        "                 n_layers=1,batch_first=True, dropout_rate=0.0, rnn_type='GRU'):\n",
        "        super(RNN, self).__init__()\n",
        "        self.batch_first = batch_first\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.rnn_type = rnn_type\n",
        "\n",
        "        self.encoder = nn.Embedding(input_size, embedding_dim=embed_dim)\n",
        "        \n",
        "        if self.rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(embed_dim, hidden_size, n_layers, dropout=self.dropout_rate, batch_first=batch_first)\n",
        "        else:\n",
        "            self.rnn = nn.LSTM(embed_dim, hidden_size, n_layers, dropout=self.dropout_rate, batch_first=batch_first)\n",
        "        self.lin1 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = torch.nn.LogSoftmax(dim=2)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \n",
        "        props = self.encoder(inputs)\n",
        "        props, hidden = self.rnn(props)\n",
        "        props = self.lin1(props)\n",
        "        props = self.relu(props)\n",
        "        props = self.decoder(props)        \n",
        "        output = self.softmax(props)\n",
        "        return output,hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyhdwVcbvNmO",
        "outputId": "0026db52-9835-4ec9-ee84-96af2c5ee0d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "f = open(\"/content/drive/My Drive/datasets/Neplai Word Predictor/65k_cleaned.txt\", \"r\", encoding=\"utf8\")\n",
        "text = f.read()\n",
        "f.close()\n",
        "sentences = text.split(\"\\n\")\n",
        "random.shuffle(sentences)\n",
        "words = []\n",
        "sens_len = []\n",
        "for s in sentences:\n",
        "    w_s = s.split()\n",
        "    sens_len.append(len(w_s))\n",
        "    \n",
        "    for w in s.split():\n",
        "        words.append(w)\n",
        "plt.hist(sens_len)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([4.3560e+03, 6.0811e+04, 1.0800e+02, 2.3000e+01, 1.2000e+01,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0000e+00]),\n",
              " array([  0. ,  11.4,  22.8,  34.2,  45.6,  57. ,  68.4,  79.8,  91.2,\n",
              "        102.6, 114. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Wj3HZQPvNm6"
      },
      "source": [
        "vocab_counter = Counter(words)\n",
        "threshold = 2\n",
        "vocab = [\"<UNK>\"]+[w for (w, c) in vocab_counter.most_common() if c >= threshold]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ageYXebEvNnX",
        "outputId": "12048946-42c7-45fc-bab4-0a70bc63510a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vocab_len = len(vocab)\n",
        "vocab_len"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46337"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jmbwh5jcvNnw"
      },
      "source": [
        "max_len = 20"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORJBq39evNoO",
        "outputId": "cafd0e13-5527-4e94-e926-c3f19d199ad6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def tensor_from_sentences(sentences,vocab, max_len=30):\n",
        "    start = time.time()\n",
        "    X = []\n",
        "    Y = []\n",
        "    vocab_len = len(vocab)\n",
        "    for s in sentences:\n",
        "        w_s = s.split()\n",
        "#         print(w_s)\n",
        "        idx_ip = [vocab.index(w) if w in vocab else 0 for w in w_s]\n",
        "        len_words = len(idx_ip)\n",
        "        \n",
        "        if len_words < max_len:\n",
        "            pad = [0] * (max_len - len_words)\n",
        "            idx_ip += pad\n",
        "        else :\n",
        "            idx_ip = idx_ip[:max_len]\n",
        "        \n",
        "        idx_op = idx_ip[1:] + [0]\n",
        "        # one_hot = []\n",
        "        # for idx in idx_op:\n",
        "        #     hot = [0] * vocab_len\n",
        "        #     hot[idx] = 1\n",
        "        #     one_hot.append(hot)\n",
        "        \n",
        "        X.append(idx_ip)\n",
        "        Y.append(idx_op)\n",
        "\n",
        "        len_X = len(X)\n",
        "        if len_X % 10000 == 0:\n",
        "            print(\"{} sentences processed in {} secs.\".format(len_X,time.time() - start))\n",
        "    return X,Y\n",
        "\n",
        "X, Y = tensor_from_sentences(sentences, vocab, max_len)\n",
        "print()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000 sentences processed in 36.13341045379639 secs.\n",
            "20000 sentences processed in 73.02827382087708 secs.\n",
            "30000 sentences processed in 110.59224128723145 secs.\n",
            "40000 sentences processed in 148.166597366333 secs.\n",
            "50000 sentences processed in 186.2036771774292 secs.\n",
            "60000 sentences processed in 224.25011229515076 secs.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnJ2MKjNvNon"
      },
      "source": [
        "def batchify(X,Y,batch_size):\n",
        "    batches_X = []\n",
        "    batches_Y = []\n",
        "    len_data = len(X)\n",
        "    for start in range(0,len_data,batch_size):\n",
        "        end  = None\n",
        "        if start + batch_size < len_data:\n",
        "            end = start + batch_size\n",
        "        else:\n",
        "            end = len_data\n",
        "        x = torch.tensor(X[start:end], dtype=torch.long)\n",
        "        y = torch.tensor(Y[start:end], dtype=torch.long)\n",
        "        \n",
        "        batches_X.append(x)\n",
        "        batches_Y.append(y)\n",
        "    \n",
        "    return batches_X, batches_Y\n",
        "\n",
        "batches_X, batches_Y = batchify(X,Y, batch_size=100)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-bsvbG4zR_7"
      },
      "source": [
        "def tensor_to_onehot(tensor, max_len):\n",
        "    assert tensor.dim() == 2\n",
        "    batch_size, seq_len = tensor.shape\n",
        "    tensor = tensor.unsqueeze(2)\n",
        "\n",
        "    onehot = torch.FloatTensor( batch_size, seq_len, max_len)\n",
        "    onehot.zero_()\n",
        "    onehot.scatter_(2,tensor,1)\n",
        "    \n",
        "    return onehot\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8YEfwHRvNpN"
      },
      "source": [
        "\n",
        "def train(model, optimizer, criterion, epochs, train_X, train_Y):\n",
        "\n",
        "    arch = \"Model: {} \\n Loss function: {} \\n Optimizer: {}\\n Batch size {}\\n \"\\\n",
        "      .format(str(model), str(criterion), str(optimizer), str(len(train_X[0])))\n",
        "\n",
        "    hist_file = open(\"/content/drive/My Drive/models/Neplai Word Predictor/history.txt\",\"a\")\n",
        "    hist_file.write(arch)\n",
        "    hist_file.close()\n",
        "    start_time = time.time()\n",
        "    for e in range(epochs):\n",
        "        train_loss = 0\n",
        "#         train_acc = 0\n",
        "\n",
        "#         test_loss = 0\n",
        "#         test_acc = 0\n",
        "        \n",
        "        for i in range(len(train_X)):\n",
        "            x = train_X[i].to(device)\n",
        "            y = train_Y[i]\n",
        "            y = tensor_to_onehot(y,vocab_len).to(device)\n",
        "            optimizer.zero_grad()\n",
        "            prediction, hidden = model(x)\n",
        "#             print(type(prediction))\n",
        "            loss = criterion(prediction,y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.data.item()\n",
        "#             train_acc += torch.eq(prediction.round(),y).sum().item()\n",
        "        train_loss /= len(train_X)\n",
        "#         train_acc /= len(train_X)\n",
        "        rec = \"Epoch {} Train Loss: {} in {} secs.\\n\".format(e+1, train_loss, time.time() - start_time)\n",
        "        print(rec)\n",
        "        hist_file = open(\"/content/drive/My Drive/models/Neplai Word Predictor/history.txt\",\"a\")\n",
        "        hist_file.write(rec)\n",
        "        hist_file.close()\n",
        "    \n",
        "\n",
        "#         for i in range(len(test_X)):\n",
        "#             x = test_X[i].to(device)\n",
        "#             y = test_Y[i:i+1].to(device)\n",
        "#             y = y.float()\n",
        "#             prediction = model(x)\n",
        "#             prediction = prediction.squeeze(1)\n",
        "#             loss = criterion(prediction,y)\n",
        "\n",
        "#             test_loss += loss.data.item()\n",
        "#             test_acc += torch.eq(prediction.round(),y).sum().item()\n",
        "#         test_loss /= len(test_X)\n",
        "#         test_acc /= len(test_X)\n",
        "\n",
        "        # if (e+1)%10 == 0:\n",
        "        #     torch.save(model, \"/content/drive/My Drive/models/SA_LSTM/model/e_\" + str(e+1)+\".bin\")\n",
        "#         print(\"Epoch: {}  Training (loss,acc): ({},{})  Test (loss, acc):({},{})\".format( e+1, train_loss, train_acc, test_loss, test_acc))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c-1HUEAvNpl",
        "outputId": "7c602ed8-6644-49a1-c81f-52f6b857074e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "device = None\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "model = RNN(input_size= vocab_len, embed_dim=50, hidden_size=1000, \n",
        "            output_size=vocab_len, n_layers=2, rnn_type=\"LSTM\").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n",
        "loss_fuction = nn.KLDivLoss()\n",
        "device"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4DEHu-WgSim",
        "outputId": "ef035756-2583-4900-cbe6-914cca1ff254",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train(model,optimizer,loss_fuction,30, batches_X, batches_Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2352: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Train Loss: 0.00013848131918419573 in 260.0845766067505 secs.\n",
            "\n",
            "Epoch 2 Train Loss: 0.00013093417858612717 in 521.7814247608185 secs.\n",
            "\n",
            "Epoch 3 Train Loss: 0.00012560847446939052 in 781.7569811344147 secs.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtMGUjHy8i0D"
      },
      "source": [
        "def tensor_from_single_sent(sentence,vocab):\n",
        "        words = sentence.split()\n",
        "        idx_ip = [vocab.index(w) if w in vocab else 0 for w in words]\n",
        "        idx_ip = torch.tensor(idx_ip, dtype=torch.long)\n",
        "        return idx_ip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhHk5aeUxZmm"
      },
      "source": [
        "def predict(model,sequence, vocab):\n",
        "    \n",
        "    assert sequence.dim() == 1\n",
        "\n",
        "    sequence = sequence.to(device)\n",
        "    sequence = torch.unsqueeze(sequence,0)\n",
        "    prediction,_ = model(sequence)\n",
        "    ip_op = []\n",
        "    prediction = torch.squeeze(prediction)  # To change from 3d to 2d \n",
        "    prediction = prediction[-1]             # To consider only the last output of predicted sequence\n",
        "    prediction[0] = float(\"-inf\")           # To ignore the \"<UNK>\" from prediction \n",
        "    print(prediction)\n",
        "    pred_word_idx = torch.argmax(prediction).item()\n",
        "\n",
        "    ip_op = sequence[0].tolist()\n",
        "    ip_op += [pred_word_idx] \n",
        "    \n",
        "    actual_words = []\n",
        "    for idx in ip_op:\n",
        "        actual_words.append(vocab[idx])\n",
        "    return actual_words\n",
        "t = 22222\n",
        "seq = tensor_from_single_sent(sentences[t], vocab)\n",
        "print(sentences[t])\n",
        "predict(model, seq[:-2], vocab)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_UZNTSqyobK"
      },
      "source": [
        "hard_ip = input(\"Enter the nepali string: \")\n",
        "seq = tensor_from_single_sent(hard_ip, vocab)\n",
        "predict(model, seq, vocab) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjzX_L91vNp8"
      },
      "source": [
        "def count_parameters(model):\n",
        "      return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "count_parameters(model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}