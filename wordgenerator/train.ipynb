{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "from model import RNN\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([262., 969., 592., 427., 172., 127.,  61.,  19.,  20.,   6.,   5.,\n",
       "          2.,   0.,   2.,   0.,   1.,   1.,   0.,   0.,   1.]),\n",
       " array([ 0. ,  3.6,  7.2, 10.8, 14.4, 18. , 21.6, 25.2, 28.8, 32.4, 36. ,\n",
       "        39.6, 43.2, 46.8, 50.4, 54. , 57.6, 61.2, 64.8, 68.4, 72. ]),\n",
       " <a list of 20 Patch objects>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD5CAYAAADLL+UrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAPt0lEQVR4nO3df6zVd33H8edroNXWOWHQBoEMTJhKzWz1htV1Ma51K1oj/acLJm5kISFZ2KyLiYMtmdkfJGxZjC5ZTUj9gdG0Y7VbifVXh5ply1Z2aessRVYmpNyB5apz/liCgu/9cb7V4+2l5Z5ze8+Bz/OR3Hy/38/5fs/3xQVe98vnnO8hVYUkqQ0/N+oAkqSFY+lLUkMsfUlqiKUvSQ2x9CWpIZa+JDVk8XPtkOQjwNuAM1X1mm5sKfC3wBrgBPDbVfU/3WM7ga3AeeBdVfX5bvz1wMeAFwOfAe6oi3i/6LJly2rNmjVz/GVJUtsOHTr0zapaPnM8z9W7Sd4IfB/4eF/p/yXw7aranWQHsKSq/jjJeuBuYAPwcuAfgV+uqvNJDgJ3AP9Gr/T/uqo++1zBJyYmanJyci6/VklqXpJDVTUxc/w5p3eq6p+Ab88Y3gTs7db3Arf1jd9TVWer6jhwDNiQZAXw0qr61+7q/uN9x0iSFsigc/rXVNVpgG55dTe+EjjZt99UN7ayW585LklaQPP9Qm5mGatnGZ/9SZJtSSaTTE5PT89bOElq3aCl/1Q3ZUO3PNONTwGr+/ZbBZzqxlfNMj6rqtpTVRNVNbF8+TNeh5AkDWjQ0t8PbOnWtwD3941vTnJFkrXAOuBgNwX0vSQ3JAnwu33HSJIWyMW8ZfNu4E3AsiRTwPuA3cC+JFuBJ4HbAarqcJJ9wOPAOWB7VZ3vnur3+elbNj/bfUmSFtBzvmVz1HzLpiTN3cBv2ZQkXT4sfUlqyHPO6bdqzY4HBj72xO5b5zGJJM0fr/QlqSGWviQ1xNKXpIZY+pLUEEtfkhpi6UtSQyx9SWqIpS9JDbH0Jakhlr4kNcTSl6SGWPqS1BBLX5IaYulLUkMsfUlqiKUvSQ2x9CWpIZa+JDXE0pekhlj6ktQQS1+SGmLpS1JDLH1JaoilL0kNsfQlqSGWviQ1xNKXpIZY+pLUEEtfkhpi6UtSQyx9SWrIUKWf5I+SHE7yWJK7k7woydIkDyZ5olsu6dt/Z5JjSY4muWX4+JKkuRi49JOsBN4FTFTVa4BFwGZgB3CgqtYBB7ptkqzvHr8W2AjcmWTRcPElSXMx7PTOYuDFSRYDVwKngE3A3u7xvcBt3fom4J6qOltVx4FjwIYhzy9JmoOBS7+q/hv4K+BJ4DTwv1X1BeCaqjrd7XMauLo7ZCVwsu8pproxSdICGWZ6Zwm9q/e1wMuBq5K889kOmWWsLvDc25JMJpmcnp4eNKIkaYZhpnfeDByvqumq+hFwH/BrwFNJVgB0yzPd/lPA6r7jV9GbDnqGqtpTVRNVNbF8+fIhIkqS+g1T+k8CNyS5MkmAm4EjwH5gS7fPFuD+bn0/sDnJFUnWAuuAg0OcX5I0R4sHPbCqHkpyL/AwcA54BNgDvATYl2QrvR8Mt3f7H06yD3i82397VZ0fMr8kaQ4GLn2Aqnof8L4Zw2fpXfXPtv8uYNcw55QkDc47ciWpIZa+JDXE0pekhlj6ktQQS1+SGmLpS1JDLH1JaoilL0kNsfQlqSGWviQ1xNKXpIZY+pLUEEtfkhpi6UtSQyx9SWqIpS9JDbH0Jakhlr4kNcTSl6SGWPqS1BBLX5IaYulLUkMsfUlqiKUvSQ2x9CWpIZa+JDXE0pekhlj6ktQQS1+SGmLpS1JDLH1JaoilL0kNsfQlqSGWviQ1ZKjST/KyJPcm+VqSI0nekGRpkgeTPNEtl/TtvzPJsSRHk9wyfHxJ0lwMe6X/QeBzVfUq4LXAEWAHcKCq1gEHum2SrAc2A9cCG4E7kywa8vySpDkYuPSTvBR4I/BhgKr6YVV9B9gE7O122wvc1q1vAu6pqrNVdRw4BmwY9PySpLkb5kr/FcA08NEkjyS5K8lVwDVVdRqgW17d7b8SONl3/FQ3JklaIMOU/mLgdcCHqup64Ad0UzkXkFnGatYdk21JJpNMTk9PDxFRktRv8RDHTgFTVfVQt30vvdJ/KsmKqjqdZAVwpm//1X3HrwJOzfbEVbUH2AMwMTEx6w+GcbZmxwNDHX9i963zlESSftbAV/pV9Q3gZJJXdkM3A48D+4Et3dgW4P5ufT+wOckVSdYC64CDg55fkjR3w1zpA/wh8MkkLwS+DvwevR8k+5JsBZ4EbgeoqsNJ9tH7wXAO2F5V54c8vyRpDoYq/ap6FJiY5aGbL7D/LmDXMOeUJA3OO3IlqSGWviQ1xNKXpIZY+pLUEEtfkhpi6UtSQyx9SWqIpS9JDbH0Jakhlr4kNcTSl6SGWPqS1BBLX5IaYulLUkMsfUlqiKUvSQ2x9CWpIZa+JDXE0pekhlj6ktQQS1+SGmLpS1JDLH1JaoilL0kNsfQlqSGWviQ1xNKXpIZY+pLUEEtfkhpi6UtSQyx9SWqIpS9JDVk86gB6pjU7Hhj42BO7b53HJJIuN0Nf6SdZlOSRJJ/utpcmeTDJE91ySd++O5McS3I0yS3DnluSNDfzMb1zB3Ckb3sHcKCq1gEHum2SrAc2A9cCG4E7kyyah/NLki7SUKWfZBVwK3BX3/AmYG+3vhe4rW/8nqo6W1XHgWPAhmHOL0mam2Gv9D8AvBf4cd/YNVV1GqBbXt2NrwRO9u031Y1JkhbIwKWf5G3Amao6dLGHzDJWF3jubUkmk0xOT08PGlGSNMMwV/o3Am9PcgK4B7gpySeAp5KsAOiWZ7r9p4DVfcevAk7N9sRVtaeqJqpqYvny5UNElCT1G7j0q2pnVa2qqjX0XqD9YlW9E9gPbOl22wLc363vBzYnuSLJWmAdcHDg5JKkOXs+3qe/G9iXZCvwJHA7QFUdTrIPeBw4B2yvqvPPw/klSRcwL6VfVV8Gvtytfwu4+QL77QJ2zcc5JUlz58cwSFJDLH1JaoilL0kNsfQlqSGWviQ1xNKXpIZY+pLUEEtfkhpi6UtSQyx9SWqIpS9JDbH0Jakhlr4kNcTSl6SGWPqS1BBLX5IaYulLUkMsfUlqiKUvSQ2x9CWpIZa+JDXE0pekhlj6ktQQS1+SGmLpS1JDFo86wPNpzY4HRh1BksaKV/qS1BBLX5IaYulLUkMsfUlqiKUvSQ2x9CWpIZa+JDXE0pekhgxc+klWJ/lSkiNJDie5oxtfmuTBJE90yyV9x+xMcizJ0SS3zMcvQJJ08Ya50j8HvKeqXg3cAGxPsh7YARyoqnXAgW6b7rHNwLXARuDOJIuGCS9JmpuBS7+qTlfVw93694AjwEpgE7C3220vcFu3vgm4p6rOVtVx4BiwYdDzS5Lmbl7m9JOsAa4HHgKuqarT0PvBAFzd7bYSONl32FQ3JklaIEOXfpKXAJ8C3l1V3322XWcZqws857Ykk0kmp6enh40oSeoMVfpJXkCv8D9ZVfd1w08lWdE9vgI4041PAav7Dl8FnJrteatqT1VNVNXE8uXLh4koSeozzLt3AnwYOFJV7+97aD+wpVvfAtzfN745yRVJ1gLrgIODnl+SNHfDfJ7+jcDvAF9N8mg39ifAbmBfkq3Ak8DtAFV1OMk+4HF67/zZXlXnhzi/JGmOBi79qvpnZp+nB7j5AsfsAnYNek5J0nC8I1eSGnJZ/3eJLRrmv4g8sfvWeUwiaRx5pS9JDbH0Jakhlr4kNcTSl6SGWPqS1BBLX5IaYulLUkMsfUlqiKUvSQ3xjlz9hHfzSpc/r/QlqSGWviQ1xNKXpIZY+pLUEEtfkhpi6UtSQyx9SWqIpS9JDbH0Jakhlr4kNcTSl6SGWPqS1BBLX5Ia4qdsal74CZ3SpcErfUlqiKUvSQ2x9CWpIZa+JDXE0pekhlj6ktQQS1+SGmLpS1JDFvzmrCQbgQ8Ci4C7qmr3QmfQeBnmxq5heWOYWrOgV/pJFgF/A7wFWA+8I8n6hcwgSS1b6OmdDcCxqvp6Vf0QuAfYtMAZJKlZCz29sxI42bc9BfzqAmeQfmKUU0vDGGZays9JattCl35mGatn7JRsA7Z1m99PcnTA8y0DvjngsQvJnPPvUsk6UM78xfOQ5NktA745gvMO4rL+vZ+DX5ptcKFLfwpY3be9Cjg1c6eq2gPsGfZkSSaramLY53m+mXP+XSpZzTn/LpWso8q50HP6/w6sS7I2yQuBzcD+Bc4gSc1a0Cv9qjqX5A+Az9N7y+ZHqurwQmaQpJYt+Pv0q+ozwGcW6HRDTxEtEHPOv0slqznn36WSdSQ5U/WM11ElSZcpP4ZBkhpyWZZ+ko1JjiY5lmTHqPP0S/KRJGeSPNY3tjTJg0me6JZLRpmxy7Q6yZeSHElyOMkd45g1yYuSHEzylS7nn49jzqclWZTkkSSf7rbHNeeJJF9N8miSyW5s7LImeVmSe5N8rfuz+oZxy5nkld338emv7yZ596hyXnalfwl81MPHgI0zxnYAB6pqHXCg2x61c8B7qurVwA3A9u77OG5ZzwI3VdVrgeuAjUluYPxyPu0O4Ejf9rjmBPiNqrqu722F45j1g8DnqupVwGvpfW/HKmdVHe2+j9cBrwf+D/h7RpWzqi6rL+ANwOf7tncCO0eda0bGNcBjfdtHgRXd+grg6KgzzpL5fuA3xzkrcCXwML27vMcuJ737Ug4ANwGfHuffe+AEsGzG2FhlBV4KHKd7bXJcc87I9lvAv4wy52V3pc/sH/WwckRZLtY1VXUaoFtePeI8PyPJGuB64CHGMGs3ZfIocAZ4sKrGMifwAeC9wI/7xsYxJ/TulP9CkkPdHfIwfllfAUwDH+2mzO5KchXjl7PfZuDubn0kOS/H0r+oj3rQxUnyEuBTwLur6rujzjObqjpfvX86rwI2JHnNqDPNlORtwJmqOjTqLBfpxqp6Hb1p0u1J3jjqQLNYDLwO+FBVXQ/8gPGYcppVd0Pq24G/G2WOy7H0L+qjHsbMU0lWAHTLMyPOA0CSF9Ar/E9W1X3d8FhmBaiq7wBfpveaybjlvBF4e5IT9D5d9qYkn2D8cgJQVae65Rl6888bGL+sU8BU9y87gHvp/RAYt5xPewvwcFU91W2PJOflWPqX4kc97Ae2dOtb6M2fj1SSAB8GjlTV+/seGqusSZYneVm3/mLgzcDXGLOcVbWzqlZV1Rp6fya/WFXvZMxyAiS5KsnPP71Obx76McYsa1V9AziZ5JXd0M3A44xZzj7v4KdTOzCqnKN+YeN5erHkrcB/Av8F/Omo88zIdjdwGvgRvSuVrcAv0nuB74luuXQMcv46vWmx/wAe7b7eOm5ZgV8BHulyPgb8WTc+VjlnZH4TP30hd+xy0psr/0r3dfjpv0NjmvU6YLL7/f8HYMmY5rwS+BbwC31jI8npHbmS1JDLcXpHknQBlr4kNcTSl6SGWPqS1BBLX5IaYulLUkMsfUlqiKUvSQ35f18J+2zNUVFuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = open(\"../preprocess/cleaned.txt\", \"r\", encoding=\"utf8\")\n",
    "text = f.read()\n",
    "sentences = text.split(\"ред\")\n",
    "words = []\n",
    "sens_len = []\n",
    "for s in sentences:\n",
    "    w_s = s.split()\n",
    "    sens_len.append(len(w_s))\n",
    "    \n",
    "    for w in s.split():\n",
    "        words.append(w)\n",
    "plt.hist(sens_len,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_counter = Counter(words)\n",
    "threshold = 3\n",
    "vocab = [\"<UNK>\"]+[w for (w, c) in vocab_counter.most_common() if c >= threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_from_sentences(sentences,vocab, max_len=30):\n",
    "    X = []\n",
    "    Y = []\n",
    "    vocab_len = len(vocab)\n",
    "    for s in sentences:\n",
    "        w_s = s.split()\n",
    "#         print(w_s)\n",
    "        idx_ip = [vocab.index(w) if w in vocab else 0 for w in w_s]\n",
    "        len_words = len(idx_ip)\n",
    "        \n",
    "        if len_words < max_len:\n",
    "            pad = [0] * (max_len - len_words)\n",
    "            idx_ip += pad\n",
    "        else :\n",
    "            idx_ip = idx_ip[:max_len]\n",
    "        \n",
    "        idx_op = idx_ip[1:] + [0]\n",
    "        one_hot = []\n",
    "        for idx in idx_op:\n",
    "            hot = [0] * vocab_len\n",
    "            hot[idx] = 1\n",
    "            one_hot.append(hot)\n",
    "        \n",
    "        X.append(idx_ip)\n",
    "        Y.append(one_hot)\n",
    "        \n",
    "    return X,Y\n",
    "\n",
    "X, Y = tensor_from_sentences(sentences, vocab, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(X,Y,batch_size):\n",
    "    batches_X = []\n",
    "    batches_Y = []\n",
    "    len_data = len(X)\n",
    "    for start in range(0,len_data,batch_size):\n",
    "        end  = None\n",
    "        if start + batch_size < len_data:\n",
    "            end = start + batch_size\n",
    "        else:\n",
    "            end = len_data\n",
    "            \n",
    "        x = torch.tensor(X[start:end], dtype=torch.long)\n",
    "        y = torch.tensor(Y[start:end], dtype=torch.float)\n",
    "        \n",
    "        batches_X.append(x)\n",
    "        batches_Y.append(y)\n",
    "    \n",
    "    return batches_X, batches_Y\n",
    "\n",
    "batches_X, batches_Y = batchify(X,Y, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(vocab_len,1000,vocab_len,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, optimizer, criterion, epochs, train_X, train_Y):\n",
    "    for e in range(epochs):\n",
    "        train_loss = 0\n",
    "#         train_acc = 0\n",
    "\n",
    "#         test_loss = 0\n",
    "#         test_acc = 0\n",
    "        \n",
    "        for i in range(len(train_X)):\n",
    "            x = train_X[i]\n",
    "            y = train_Y[i]\n",
    "            optimizer.zero_grad()\n",
    "            prediction, hidden = model(x)\n",
    "#             print(type(prediction))\n",
    "            loss = criterion(prediction,y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.data.item()\n",
    "#             train_acc += torch.eq(prediction.round(),y).sum().item()\n",
    "        train_loss /= len(train_X)\n",
    "#         train_acc /= len(train_X)\n",
    "        print(\"Epoch {} Train Loss: {}\".format(e+1, train_loss))\n",
    "\n",
    "#         for i in range(len(test_X)):\n",
    "#             x = test_X[i].to(device)\n",
    "#             y = test_Y[i:i+1].to(device)\n",
    "#             y = y.float()\n",
    "#             prediction = model(x)\n",
    "#             prediction = prediction.squeeze(1)\n",
    "#             loss = criterion(prediction,y)\n",
    "\n",
    "#             test_loss += loss.data.item()\n",
    "#             test_acc += torch.eq(prediction.round(),y).sum().item()\n",
    "#         test_loss /= len(test_X)\n",
    "#         test_acc /= len(test_X)\n",
    "\n",
    "#         if (e+1)%10 == 0:\n",
    "#             torch.save(model, \"/content/drive/My Drive/models/SA_LSTM/model/e_\" + str(e+1)+\".bin\")\n",
    "#         print(\"Epoch: {}  Training (loss,acc): ({},{})  Test (loss, acc):({},{})\".format( e+1, train_loss, train_acc, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mnsdk\\miniconda3\\lib\\site-packages\\torch\\nn\\functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train Loss: 0.0009904016374433135\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fuction = nn.KLDivLoss()\n",
    "train(model,optimizer,loss_fuction,1, batches_X,batches_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "      return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_parameters(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
