{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "train.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeMcxojBvbyl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "895cad94-6c1f-4954-d3f4-06698ab351bc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQbl5K9FvNle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGzJPrz3wE7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1,batch_first=True, dropout_rate=0.0, rnn_type='GRU'):\n",
        "        super(RNN, self).__init__()\n",
        "        self.batch_first = batch_first\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.rnn_type = rnn_type\n",
        "\n",
        "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
        "        self.softmax = torch.nn.LogSoftmax(dim=2)\n",
        "        \n",
        "        if self.rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(hidden_size, hidden_size, n_layers, dropout=self.dropout_rate, batch_first=batch_first)\n",
        "        else:\n",
        "            self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers, dropout=self.dropout_rate, batch_first=batch_first)\n",
        "\n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \n",
        "        inputs = self.encoder(inputs)\n",
        "        output, hidden = self.rnn(inputs)\n",
        "        output = self.decoder(output)        \n",
        "        output = self.softmax(output)\n",
        "        return output,hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyhdwVcbvNmO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "7fbd8d5f-ca9f-4b26-a1ea-5273c41df402"
      },
      "source": [
        "f = open(\"/content/drive/My Drive/datasets/Neplai Word Predictor/cleaned.txt\", \"r\", encoding=\"utf8\")\n",
        "text = f.read()\n",
        "f.close()\n",
        "sentences = text.split(\"।\")\n",
        "words = []\n",
        "sens_len = []\n",
        "for s in sentences:\n",
        "    w_s = s.split()\n",
        "    sens_len.append(len(w_s))\n",
        "    \n",
        "    for w in s.split():\n",
        "        words.append(w)\n",
        "plt.hist(sens_len,20)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([262., 969., 592., 427., 172., 127.,  61.,  19.,  20.,   6.,   5.,\n",
              "          2.,   0.,   2.,   0.,   1.,   1.,   0.,   0.,   1.]),\n",
              " array([ 0. ,  3.6,  7.2, 10.8, 14.4, 18. , 21.6, 25.2, 28.8, 32.4, 36. ,\n",
              "        39.6, 43.2, 46.8, 50.4, 54. , 57.6, 61.2, 64.8, 68.4, 72. ]),\n",
              " <a list of 20 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Wj3HZQPvNm6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_counter = Counter(words)\n",
        "threshold = 2\n",
        "vocab = [\"<UNK>\"]+[w for (w, c) in vocab_counter.most_common() if c >= threshold]"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ageYXebEvNnX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7f9a5af6-59f7-4c87-8b28-4232907bea50"
      },
      "source": [
        "vocab_len = len(vocab)\n",
        "vocab_len"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2859"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jmbwh5jcvNnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len = 30"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORJBq39evNoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tensor_from_sentences(sentences,vocab, max_len=30):\n",
        "    X = []\n",
        "    Y = []\n",
        "    vocab_len = len(vocab)\n",
        "    for s in sentences:\n",
        "        w_s = s.split()\n",
        "#         print(w_s)\n",
        "        idx_ip = [vocab.index(w) if w in vocab else 0 for w in w_s]\n",
        "        len_words = len(idx_ip)\n",
        "        \n",
        "        if len_words < max_len:\n",
        "            pad = [0] * (max_len - len_words)\n",
        "            idx_ip += pad\n",
        "        else :\n",
        "            idx_ip = idx_ip[:max_len]\n",
        "        \n",
        "        idx_op = idx_ip[1:] + [0]\n",
        "        one_hot = []\n",
        "        for idx in idx_op:\n",
        "            hot = [0] * vocab_len\n",
        "            hot[idx] = 1\n",
        "            one_hot.append(hot)\n",
        "        \n",
        "        X.append(idx_ip)\n",
        "        Y.append(one_hot)\n",
        "        \n",
        "    return X,Y\n",
        "\n",
        "X, Y = tensor_from_sentences(sentences, vocab, max_len)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnJ2MKjNvNon",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batchify(X,Y,batch_size):\n",
        "    batches_X = []\n",
        "    batches_Y = []\n",
        "    len_data = len(X)\n",
        "    for start in range(0,len_data,batch_size):\n",
        "        end  = None\n",
        "        if start + batch_size < len_data:\n",
        "            end = start + batch_size\n",
        "        else:\n",
        "            end = len_data\n",
        "            \n",
        "        x = torch.tensor(X[start:end], dtype=torch.long)\n",
        "        y = torch.tensor(Y[start:end], dtype=torch.float)\n",
        "        \n",
        "        batches_X.append(x)\n",
        "        batches_Y.append(y)\n",
        "    \n",
        "    return batches_X, batches_Y\n",
        "\n",
        "batches_X, batches_Y = batchify(X,Y, batch_size=100)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kl6X4KRvNo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = None\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "\n",
        "model = RNN(vocab_len,1000,vocab_len,2, rnn_type=\"LSTM\").to(device)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8YEfwHRvNpN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train(model, optimizer, criterion, epochs, train_X, train_Y):\n",
        "\n",
        "    arch = \"Model: {} \\n Loss function: {} \\n Optimizer: {}\\n Batch size {}\\n \"\\\n",
        "      .format(str(model), str(criterion), str(optimizer), str(len(train_X[0])))\n",
        "      \n",
        "    hist_file = open(\"/content/drive/My Drive/models/Neplai Word Predictor/history.txt\",\"a\")\n",
        "    hist_file.write(arch)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for e in range(epochs):\n",
        "        train_loss = 0\n",
        "#         train_acc = 0\n",
        "\n",
        "#         test_loss = 0\n",
        "#         test_acc = 0\n",
        "        \n",
        "        for i in range(len(train_X)):\n",
        "            x = train_X[i].to(device)\n",
        "            y = train_Y[i].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            prediction, hidden = model(x)\n",
        "#             print(type(prediction))\n",
        "            loss = criterion(prediction,y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.data.item()\n",
        "#             train_acc += torch.eq(prediction.round(),y).sum().item()\n",
        "        train_loss /= len(train_X)\n",
        "#         train_acc /= len(train_X)\n",
        "        rec = \"Epoch {} Train Loss: {} in {} secs.\\n\".format(e+1, train_loss, time.time() - start_time)\n",
        "        print(rec)\n",
        "        hist_file.write(rec)\n",
        "    hist_file.close()\n",
        "\n",
        "#         for i in range(len(test_X)):\n",
        "#             x = test_X[i].to(device)\n",
        "#             y = test_Y[i:i+1].to(device)\n",
        "#             y = y.float()\n",
        "#             prediction = model(x)\n",
        "#             prediction = prediction.squeeze(1)\n",
        "#             loss = criterion(prediction,y)\n",
        "\n",
        "#             test_loss += loss.data.item()\n",
        "#             test_acc += torch.eq(prediction.round(),y).sum().item()\n",
        "#         test_loss /= len(test_X)\n",
        "#         test_acc /= len(test_X)\n",
        "\n",
        "        # if (e+1)%10 == 0:\n",
        "        #     torch.save(model, \"/content/drive/My Drive/models/SA_LSTM/model/e_\" + str(e+1)+\".bin\")\n",
        "#         print(\"Epoch: {}  Training (loss,acc): ({},{})  Test (loss, acc):({},{})\".format( e+1, train_loss, train_acc, test_loss, test_acc))"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c-1HUEAvNpl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fcbe25c0-c4cc-42c3-84d4-23c8207afc48"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "loss_fuction = nn.KLDivLoss()\n",
        "train(model,optimizer,loss_fuction,30, batches_X,batches_Y)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2352: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Train Loss: 7.210669018267827e-05 in 7.044266223907471 secs.\n",
            "Epoch 2 Train Loss: 5.912860096604736e-05 in 14.098771810531616 secs.\n",
            "Epoch 3 Train Loss: 5.2923477697832925e-05 in 21.150527477264404 secs.\n",
            "Epoch 4 Train Loss: 4.895264013342697e-05 in 28.190850496292114 secs.\n",
            "Epoch 5 Train Loss: 4.665198017998288e-05 in 35.23406529426575 secs.\n",
            "Epoch 6 Train Loss: 4.49685347255716e-05 in 42.26276206970215 secs.\n",
            "Epoch 7 Train Loss: 4.414337847804375e-05 in 49.27171564102173 secs.\n",
            "Epoch 8 Train Loss: 4.256852574784447e-05 in 56.303473234176636 secs.\n",
            "Epoch 9 Train Loss: 3.952604594006617e-05 in 63.3070170879364 secs.\n",
            "Epoch 10 Train Loss: 3.743122511810658e-05 in 70.31932425498962 secs.\n",
            "Epoch 11 Train Loss: 3.61713574184924e-05 in 77.33484935760498 secs.\n",
            "Epoch 12 Train Loss: 3.535763223933625e-05 in 84.36265540122986 secs.\n",
            "Epoch 13 Train Loss: 3.481324643804261e-05 in 91.37777280807495 secs.\n",
            "Epoch 14 Train Loss: 3.446538465612792e-05 in 98.36888027191162 secs.\n",
            "Epoch 15 Train Loss: 3.414509302712287e-05 in 105.38285493850708 secs.\n",
            "Epoch 16 Train Loss: 3.3901948107221096e-05 in 112.40000605583191 secs.\n",
            "Epoch 17 Train Loss: 3.368732568223668e-05 in 119.40182995796204 secs.\n",
            "Epoch 18 Train Loss: 3.350208835736676e-05 in 126.39952373504639 secs.\n",
            "Epoch 19 Train Loss: 3.329410328997592e-05 in 133.40057635307312 secs.\n",
            "Epoch 20 Train Loss: 3.317752998555079e-05 in 140.40468525886536 secs.\n",
            "Epoch 21 Train Loss: 3.300794048333989e-05 in 147.42530393600464 secs.\n",
            "Epoch 22 Train Loss: 3.291129319728093e-05 in 154.42448043823242 secs.\n",
            "Epoch 23 Train Loss: 3.2747485251834235e-05 in 161.4404661655426 secs.\n",
            "Epoch 24 Train Loss: 3.264236639316315e-05 in 168.43197202682495 secs.\n",
            "Epoch 25 Train Loss: 3.2542773624300025e-05 in 175.4276373386383 secs.\n",
            "Epoch 26 Train Loss: 3.246219219161301e-05 in 182.42374849319458 secs.\n",
            "Epoch 27 Train Loss: 3.235490907240159e-05 in 189.42891144752502 secs.\n",
            "Epoch 28 Train Loss: 3.229229184432404e-05 in 196.4399790763855 secs.\n",
            "Epoch 29 Train Loss: 3.219472953922512e-05 in 203.4414038658142 secs.\n",
            "Epoch 30 Train Loss: 3.2122756103355506e-05 in 210.4433000087738 secs.\n",
            "Epoch 31 Train Loss: 3.206254236660984e-05 in 217.45040369033813 secs.\n",
            "Epoch 32 Train Loss: 3.1991583253127625e-05 in 224.45114254951477 secs.\n",
            "Epoch 33 Train Loss: 3.1929673207708186e-05 in 231.4677119255066 secs.\n",
            "Epoch 34 Train Loss: 3.18878148930337e-05 in 238.46528911590576 secs.\n",
            "Epoch 35 Train Loss: 3.1822485753865396e-05 in 245.48014783859253 secs.\n",
            "Epoch 36 Train Loss: 3.178271085575775e-05 in 252.4694185256958 secs.\n",
            "Epoch 37 Train Loss: 3.1719727943440104e-05 in 259.48114466667175 secs.\n",
            "Epoch 38 Train Loss: 3.1699846928219175e-05 in 266.4917097091675 secs.\n",
            "Epoch 39 Train Loss: 3.1656517118487194e-05 in 273.5027902126312 secs.\n",
            "Epoch 40 Train Loss: 3.1620419745363676e-05 in 280.5191731452942 secs.\n",
            "Epoch 41 Train Loss: 3.1561608382078166e-05 in 287.5310950279236 secs.\n",
            "Epoch 42 Train Loss: 3.150645507385316e-05 in 294.5379843711853 secs.\n",
            "Epoch 43 Train Loss: 3.146474982376849e-05 in 301.54675221443176 secs.\n",
            "Epoch 44 Train Loss: 3.143858810761702e-05 in 308.5265746116638 secs.\n",
            "Epoch 45 Train Loss: 3.1422097688644295e-05 in 315.53544092178345 secs.\n",
            "Epoch 46 Train Loss: 3.140084966920161e-05 in 322.54207277297974 secs.\n",
            "Epoch 47 Train Loss: 3.1355748298530535e-05 in 329.54664731025696 secs.\n",
            "Epoch 48 Train Loss: 3.1311505154008046e-05 in 336.56225514411926 secs.\n",
            "Epoch 49 Train Loss: 3.129153285246705e-05 in 343.5708236694336 secs.\n",
            "Epoch 50 Train Loss: 3.1265274056177416e-05 in 350.58108830451965 secs.\n",
            "Epoch 51 Train Loss: 3.1239265404615755e-05 in 357.5803201198578 secs.\n",
            "Epoch 52 Train Loss: 3.119705737238595e-05 in 364.5702600479126 secs.\n",
            "Epoch 53 Train Loss: 3.118586829697489e-05 in 371.5835564136505 secs.\n",
            "Epoch 54 Train Loss: 3.114998994364955e-05 in 378.5981252193451 secs.\n",
            "Epoch 55 Train Loss: 3.113514752907644e-05 in 385.6077837944031 secs.\n",
            "Epoch 56 Train Loss: 3.1110917108081696e-05 in 392.61138820648193 secs.\n",
            "Epoch 57 Train Loss: 3.1089131206735384e-05 in 399.6203842163086 secs.\n",
            "Epoch 58 Train Loss: 3.10585139770949e-05 in 406.63811135292053 secs.\n",
            "Epoch 59 Train Loss: 3.1026890374724407e-05 in 413.65405011177063 secs.\n",
            "Epoch 60 Train Loss: 3.100755478684463e-05 in 420.662526845932 secs.\n",
            "Epoch 61 Train Loss: 3.0996903323857284e-05 in 427.6520071029663 secs.\n",
            "Epoch 62 Train Loss: 3.098215408196362e-05 in 434.66013860702515 secs.\n",
            "Epoch 63 Train Loss: 3.094656164682453e-05 in 441.659597158432 secs.\n",
            "Epoch 64 Train Loss: 3.092230488416842e-05 in 448.6585223674774 secs.\n",
            "Epoch 65 Train Loss: 3.090272494537877e-05 in 455.6708812713623 secs.\n",
            "Epoch 66 Train Loss: 3.0897486255896556e-05 in 462.6782486438751 secs.\n",
            "Epoch 67 Train Loss: 3.088698165946106e-05 in 469.6665303707123 secs.\n",
            "Epoch 68 Train Loss: 3.08789746028466e-05 in 476.6617856025696 secs.\n",
            "Epoch 69 Train Loss: 3.08516547294049e-05 in 483.67454838752747 secs.\n",
            "Epoch 70 Train Loss: 3.0830160876912826e-05 in 490.6883945465088 secs.\n",
            "Epoch 71 Train Loss: 3.08138424534937e-05 in 497.6970114707947 secs.\n",
            "Epoch 72 Train Loss: 3.080108183860796e-05 in 504.7018451690674 secs.\n",
            "Epoch 73 Train Loss: 3.0788261062331946e-05 in 511.71526074409485 secs.\n",
            "Epoch 74 Train Loss: 3.077040087696837e-05 in 518.7319021224976 secs.\n",
            "Epoch 75 Train Loss: 3.075186254540717e-05 in 525.7530946731567 secs.\n",
            "Epoch 76 Train Loss: 3.073784689520296e-05 in 532.7579100131989 secs.\n",
            "Epoch 77 Train Loss: 3.072568472783098e-05 in 539.7603888511658 secs.\n",
            "Epoch 78 Train Loss: 3.071727803250758e-05 in 546.7679908275604 secs.\n",
            "Epoch 79 Train Loss: 3.070449201914016e-05 in 553.77450299263 secs.\n",
            "Epoch 80 Train Loss: 3.0686058447894606e-05 in 560.7828888893127 secs.\n",
            "Epoch 81 Train Loss: 3.067509869988835e-05 in 567.7852718830109 secs.\n",
            "Epoch 82 Train Loss: 3.066375352823848e-05 in 574.8026392459869 secs.\n",
            "Epoch 83 Train Loss: 3.06603317396905e-05 in 581.8030323982239 secs.\n",
            "Epoch 84 Train Loss: 3.0652915315672375e-05 in 588.8106892108917 secs.\n",
            "Epoch 85 Train Loss: 3.063391873352143e-05 in 595.8230605125427 secs.\n",
            "Epoch 86 Train Loss: 3.062036355711623e-05 in 602.8341279029846 secs.\n",
            "Epoch 87 Train Loss: 3.0607943699578755e-05 in 609.8244571685791 secs.\n",
            "Epoch 88 Train Loss: 3.059865593968425e-05 in 616.8362681865692 secs.\n",
            "Epoch 89 Train Loss: 3.0589390681362364e-05 in 623.8437511920929 secs.\n",
            "Epoch 90 Train Loss: 3.05806700419419e-05 in 630.8616440296173 secs.\n",
            "Epoch 91 Train Loss: 3.056954011737145e-05 in 637.8633031845093 secs.\n",
            "Epoch 92 Train Loss: 3.055717570532579e-05 in 644.8657283782959 secs.\n",
            "Epoch 93 Train Loss: 3.0543851271094924e-05 in 651.874489068985 secs.\n",
            "Epoch 94 Train Loss: 3.0537607421542314e-05 in 658.8733494281769 secs.\n",
            "Epoch 95 Train Loss: 3.052958648671241e-05 in 665.8908722400665 secs.\n",
            "Epoch 96 Train Loss: 3.052249310173836e-05 in 672.9078760147095 secs.\n",
            "Epoch 97 Train Loss: 3.05119987455395e-05 in 679.9102234840393 secs.\n",
            "Epoch 98 Train Loss: 3.049473532344025e-05 in 686.9112102985382 secs.\n",
            "Epoch 99 Train Loss: 3.0486088319405638e-05 in 693.9054796695709 secs.\n",
            "Epoch 100 Train Loss: 3.0477863792502495e-05 in 700.896680355072 secs.\n",
            "Epoch 101 Train Loss: 3.046747850829787e-05 in 707.9000148773193 secs.\n",
            "Epoch 102 Train Loss: 3.0461306003551862e-05 in 714.900309085846 secs.\n",
            "Epoch 103 Train Loss: 3.0451147351952926e-05 in 721.9080235958099 secs.\n",
            "Epoch 104 Train Loss: 3.0447288197508357e-05 in 728.9079647064209 secs.\n",
            "Epoch 105 Train Loss: 3.0451084563133514e-05 in 735.9171905517578 secs.\n",
            "Epoch 106 Train Loss: 3.0456292666645755e-05 in 742.9227962493896 secs.\n",
            "Epoch 107 Train Loss: 3.0454465255994972e-05 in 749.9319825172424 secs.\n",
            "Epoch 108 Train Loss: 3.045057113653187e-05 in 756.9260137081146 secs.\n",
            "Epoch 109 Train Loss: 3.0408442129286145e-05 in 763.9132559299469 secs.\n",
            "Epoch 110 Train Loss: 3.041347197183684e-05 in 770.9215364456177 secs.\n",
            "Epoch 111 Train Loss: 3.039872704162176e-05 in 777.9256594181061 secs.\n",
            "Epoch 112 Train Loss: 3.03931722522032e-05 in 784.9253880977631 secs.\n",
            "Epoch 113 Train Loss: 3.0374500258605825e-05 in 791.9238390922546 secs.\n",
            "Epoch 114 Train Loss: 3.0368426046657583e-05 in 798.9443151950836 secs.\n",
            "Epoch 115 Train Loss: 3.035553109603589e-05 in 805.9634873867035 secs.\n",
            "Epoch 116 Train Loss: 3.035356854120942e-05 in 812.9628422260284 secs.\n",
            "Epoch 117 Train Loss: 3.0340284596335282e-05 in 819.9817292690277 secs.\n",
            "Epoch 118 Train Loss: 3.0344175751519355e-05 in 826.9829263687134 secs.\n",
            "Epoch 119 Train Loss: 3.0329350044700766e-05 in 833.9818663597107 secs.\n",
            "Epoch 120 Train Loss: 3.0335727421549598e-05 in 840.9717118740082 secs.\n",
            "Epoch 121 Train Loss: 3.031855953007902e-05 in 847.9799575805664 secs.\n",
            "Epoch 122 Train Loss: 3.0322787939372506e-05 in 854.9758815765381 secs.\n",
            "Epoch 123 Train Loss: 3.0302438500695172e-05 in 861.9670271873474 secs.\n",
            "Epoch 124 Train Loss: 3.0307035019547954e-05 in 868.9753863811493 secs.\n",
            "Epoch 125 Train Loss: 3.0288635941731088e-05 in 875.9644119739532 secs.\n",
            "Epoch 126 Train Loss: 3.0294418576414938e-05 in 882.9613897800446 secs.\n",
            "Epoch 127 Train Loss: 3.027485244847075e-05 in 889.9635632038116 secs.\n",
            "Epoch 128 Train Loss: 3.028302691948031e-05 in 896.956652879715 secs.\n",
            "Epoch 129 Train Loss: 3.026389526052366e-05 in 903.9739108085632 secs.\n",
            "Epoch 130 Train Loss: 3.0271836833519586e-05 in 910.9747667312622 secs.\n",
            "Epoch 131 Train Loss: 3.025107913277612e-05 in 917.9707083702087 secs.\n",
            "Epoch 132 Train Loss: 3.02587000246772e-05 in 924.9753935337067 secs.\n",
            "Epoch 133 Train Loss: 3.0239813659810118e-05 in 931.99107670784 secs.\n",
            "Epoch 134 Train Loss: 3.0246481468633715e-05 in 939.0080115795135 secs.\n",
            "Epoch 135 Train Loss: 3.0228089936254373e-05 in 946.003746509552 secs.\n",
            "Epoch 136 Train Loss: 3.0234803758772882e-05 in 953.0113272666931 secs.\n",
            "Epoch 137 Train Loss: 3.0219080616478567e-05 in 960.0206837654114 secs.\n",
            "Epoch 138 Train Loss: 3.02245699222786e-05 in 967.019917011261 secs.\n",
            "Epoch 139 Train Loss: 3.0209664516776144e-05 in 974.0186729431152 secs.\n",
            "Epoch 140 Train Loss: 3.021381956016369e-05 in 981.0174911022186 secs.\n",
            "Epoch 141 Train Loss: 3.020078434524799e-05 in 988.0233175754547 secs.\n",
            "Epoch 142 Train Loss: 3.020333173885269e-05 in 995.015638589859 secs.\n",
            "Epoch 143 Train Loss: 3.019127500549614e-05 in 1002.0241837501526 secs.\n",
            "Epoch 144 Train Loss: 3.019277499805429e-05 in 1009.0241456031799 secs.\n",
            "Epoch 145 Train Loss: 3.0182711270754226e-05 in 1016.0395076274872 secs.\n",
            "Epoch 146 Train Loss: 3.018363046673282e-05 in 1023.0436940193176 secs.\n",
            "Epoch 147 Train Loss: 3.0173655465693662e-05 in 1030.0673568248749 secs.\n",
            "Epoch 148 Train Loss: 3.0173866468464473e-05 in 1037.0769171714783 secs.\n",
            "Epoch 149 Train Loss: 3.016502328305419e-05 in 1044.0875544548035 secs.\n",
            "Epoch 150 Train Loss: 3.0164577967500212e-05 in 1051.0871670246124 secs.\n",
            "Epoch 151 Train Loss: 3.015605977486336e-05 in 1058.0863161087036 secs.\n",
            "Epoch 152 Train Loss: 3.015494399328923e-05 in 1065.1044528484344 secs.\n",
            "Epoch 153 Train Loss: 3.0147790985884733e-05 in 1072.1142234802246 secs.\n",
            "Epoch 154 Train Loss: 3.014608241587498e-05 in 1079.1055171489716 secs.\n",
            "Epoch 155 Train Loss: 3.013963665849857e-05 in 1086.108894586563 secs.\n",
            "Epoch 156 Train Loss: 3.013744726895648e-05 in 1093.1164047718048 secs.\n",
            "Epoch 157 Train Loss: 3.013127051990186e-05 in 1100.1209137439728 secs.\n",
            "Epoch 158 Train Loss: 3.012897293417525e-05 in 1107.1283152103424 secs.\n",
            "Epoch 159 Train Loss: 3.0121877460731883e-05 in 1114.1336798667908 secs.\n",
            "Epoch 160 Train Loss: 3.0120168014912417e-05 in 1121.1278941631317 secs.\n",
            "Epoch 161 Train Loss: 3.011204420612625e-05 in 1128.1293530464172 secs.\n",
            "Epoch 162 Train Loss: 3.0111244524488467e-05 in 1135.1272888183594 secs.\n",
            "Epoch 163 Train Loss: 3.010217888341113e-05 in 1142.1526799201965 secs.\n",
            "Epoch 164 Train Loss: 3.0102321168804473e-05 in 1149.154480457306 secs.\n",
            "Epoch 165 Train Loss: 3.009250623883283e-05 in 1156.1475851535797 secs.\n",
            "Epoch 166 Train Loss: 3.0093519483300582e-05 in 1163.1581356525421 secs.\n",
            "Epoch 167 Train Loss: 3.0083294404903427e-05 in 1170.1540367603302 secs.\n",
            "Epoch 168 Train Loss: 3.0085145125566567e-05 in 1177.1532106399536 secs.\n",
            "Epoch 169 Train Loss: 3.0074686677565938e-05 in 1184.1620633602142 secs.\n",
            "Epoch 170 Train Loss: 3.0077254416903964e-05 in 1191.1640994548798 secs.\n",
            "Epoch 171 Train Loss: 3.0066744094020344e-05 in 1198.1708416938782 secs.\n",
            "Epoch 172 Train Loss: 3.0069885892940135e-05 in 1205.165756225586 secs.\n",
            "Epoch 173 Train Loss: 3.005948161040174e-05 in 1212.1711027622223 secs.\n",
            "Epoch 174 Train Loss: 3.006289214816341e-05 in 1219.193558216095 secs.\n",
            "Epoch 175 Train Loss: 3.0052706885423202e-05 in 1226.196096420288 secs.\n",
            "Epoch 176 Train Loss: 3.005606164084316e-05 in 1233.1980197429657 secs.\n",
            "Epoch 177 Train Loss: 3.0046083742894956e-05 in 1240.1945424079895 secs.\n",
            "Epoch 178 Train Loss: 3.0048931876081042e-05 in 1247.1933212280273 secs.\n",
            "Epoch 179 Train Loss: 3.0039204392340724e-05 in 1254.181450843811 secs.\n",
            "Epoch 180 Train Loss: 3.0041362050623218e-05 in 1261.170550107956 secs.\n",
            "Epoch 181 Train Loss: 3.003211949598389e-05 in 1268.1640694141388 secs.\n",
            "Epoch 182 Train Loss: 3.0033415357508972e-05 in 1275.1690030097961 secs.\n",
            "Epoch 183 Train Loss: 3.0024881939257114e-05 in 1282.178418636322 secs.\n",
            "Epoch 184 Train Loss: 3.002549682504549e-05 in 1289.1906826496124 secs.\n",
            "Epoch 185 Train Loss: 3.0017858888540002e-05 in 1296.1993234157562 secs.\n",
            "Epoch 186 Train Loss: 3.0017799535589466e-05 in 1303.20437002182 secs.\n",
            "Epoch 187 Train Loss: 3.001096855667937e-05 in 1310.2031273841858 secs.\n",
            "Epoch 188 Train Loss: 3.001059512489182e-05 in 1317.2078835964203 secs.\n",
            "Epoch 189 Train Loss: 3.0004559785420833e-05 in 1324.2068243026733 secs.\n",
            "Epoch 190 Train Loss: 3.0003771691419252e-05 in 1331.2185933589935 secs.\n",
            "Epoch 191 Train Loss: 2.9998225121137995e-05 in 1338.2126252651215 secs.\n",
            "Epoch 192 Train Loss: 2.999746134769844e-05 in 1345.2142803668976 secs.\n",
            "Epoch 193 Train Loss: 2.99921861436882e-05 in 1352.2119617462158 secs.\n",
            "Epoch 194 Train Loss: 2.999124862307562e-05 in 1359.2218041419983 secs.\n",
            "Epoch 195 Train Loss: 2.9985906385937362e-05 in 1366.224761724472 secs.\n",
            "Epoch 196 Train Loss: 2.9985188760932674e-05 in 1373.2377293109894 secs.\n",
            "Epoch 197 Train Loss: 2.997950414956668e-05 in 1380.2353963851929 secs.\n",
            "Epoch 198 Train Loss: 2.9978976912119565e-05 in 1387.2347061634064 secs.\n",
            "Epoch 199 Train Loss: 2.9972702274487045e-05 in 1394.2247936725616 secs.\n",
            "Epoch 200 Train Loss: 2.997295848251303e-05 in 1401.2204270362854 secs.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtMGUjHy8i0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tensor_from_single_sent(sentence,vocab):\n",
        "        words = sentence.split()\n",
        "#         print(w_s)\n",
        "        idx_ip = [vocab.index(w) if w in vocab else 0 for w in words]\n",
        "        idx_ip = torch.tensor(idx_ip, dtype=torch.long)\n",
        "        return idx_ip"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhHk5aeUxZmm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b2f90d8b-5dc8-4146-d40a-3e867aefa598"
      },
      "source": [
        "def predict(model,sequence, vocab):\n",
        "    sequence = sequence.to(device)\n",
        "    sequence = torch.unsqueeze(sequence,0)\n",
        "    prediction,_ = model(sequence)\n",
        "    ip_op = []\n",
        "    for i  in range(len(prediction)):\n",
        "        sentence = prediction[i]\n",
        "        seq = sequence[i].tolist()\n",
        "        words_idx = torch.argmax(sentence, dim=1)\n",
        "        words_idx = words_idx.tolist()\n",
        "        seq += words_idx[-1:]\n",
        "        ip_op.append(seq)\n",
        "    \n",
        "    actual_words = []\n",
        "    for seq in ip_op:\n",
        "        actual_words.append([vocab[idx] for idx in seq])\n",
        "    return actual_words\n",
        "t = 1941\n",
        "seq = tensor_from_single_sent(sentences[t], vocab)\n",
        "print(sentences[t])\n",
        "predict(model, seq[:-2], vocab)  "
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " समुह वीचबाट अर्को बोल्यो \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<UNK>', '<UNK>', '<UNK>']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_UZNTSqyobK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "04c245c1-24ea-4f19-8d61-e0ded9951a75"
      },
      "source": [
        "hard_ip = input(\"Enter the nepali string: \")\n",
        "seq = tensor_from_single_sent(hard_ip, vocab)\n",
        "predict(model, seq, vocab) "
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the nepali string: म खाटमा\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['म', '<UNK>', '<UNK>']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjzX_L91vNp8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9aec7737-f12e-486d-96b7-02e1a0a7515c"
      },
      "source": [
        "def count_parameters(model):\n",
        "      return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "count_parameters(model)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21736859"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    }
  ]
}